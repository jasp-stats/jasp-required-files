<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Report performance measures using an object of class...</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for summary.hmeasure {hmeasure}"><tr><td>summary.hmeasure {hmeasure}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>
Report performance measures using an object of class &quot;hmeasure&quot;
</h2>

<h3>Description</h3>

<p>This function retrieves a convenient numeric summary of the output of the HMeasure function. 
</p>


<h3>Usage</h3>

<pre>
## S3 method for class 'hmeasure'
summary(object, show.all, ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>object</code></td>
<td>

<p>an object of class hmeasure
</p>
</td></tr>
<tr valign="top"><td><code>show.all</code></td>
<td>

<p>when this is FALSE only aggregate metrics are reported, whereas when TRUE, threshold-specific metrics are additionally reported &ndash; see the package vignette for more details. By default this is set to FALSE. 
</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
 
<p>additional arguments affecting the summary produced
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Objects of class &quot;hmeasure&quot; have a field called &quot;metrics&quot;, which reports several performance metrics in the form of a data frame with one row per classifier. Please refer to help(HMeasure) or the package vignette to find out more information about the measures reported. The summary method for hmeasure objects retrieves and prints this field. By default only the most important aggregate metrics are reported. Additionally setting show.all=TRUE will report all available metrics. 
</p>


<h3>Value</h3>

<p>The summary method returns the &quot;metrics&quot; field of the original hmeasure object: i.e., a data frame where each row is a classifier, and each column a performance metric. 
</p>


<h3>Author(s)</h3>

<p>Christoforos Anagnostopoulos &lt;canagnos@imperial.ac.uk&gt; and David J. Hand &lt;d.j.hand@imperial.ac.uk&gt;
</p>
<p>Maintainer: Christoforos Anagnostopoulos &lt;canagnos@imperial.ac.uk&gt;
</p>


<h3>References</h3>

<p>Hand, D.J. 2009. Measuring classifier performance: a coherent alternative to the area under the ROC curve. <em>Machine Learning</em>, <b>77</b>, 103&ndash;123.
</p>
<p>Hand, D.J. 2010. Evaluating diagnostic tests: the area under the ROC curve and the balance of errors. <em>Statistics in Medicine</em>, <b>29</b>, 1502&ndash;1510.
</p>
<p>Hand, D.J. and Anagnostopoulos, C. 2012. A better Beta for the H measure of classification performance. Preprint, arXiv:1202.2564v1
</p>


<h3>See Also</h3>

<p>plotROC, misclassCounts, relabel, HMeasure
</p>


<h3>Examples</h3>

<pre>



# load the data
library(MASS) 
library(class) 
data(Pima.te) 

# split it into training and test
n &lt;- dim(Pima.te)[1] 
ntrain &lt;- floor(2*n/3) 
ntest &lt;- n-ntrain
pima.train &lt;- Pima.te[seq(1,n,3),]
pima.test &lt;- Pima.te[-seq(1,n,3),]
true.class&lt;-pima.test[,8]

# train an LDA classifier
pima.lda &lt;- lda(formula=type~., data=pima.train)
out.lda &lt;- predict(pima.lda,newdata=pima.test) 

# obtain the predicted labels and classification scores
scores.lda &lt;- out.lda$posterior[,2]

# train k-NN classifier
class.knn &lt;- knn(train=pima.train[,-8], test=pima.test[,-8],
  cl=pima.train$type, k=9, prob=TRUE, use.all=TRUE)
scores.knn &lt;- attr(class.knn,"prob")
# this is necessary because k-NN by default outputs
# the posterior probability of the winning class
scores.knn[class.knn=="No"] &lt;- 1-scores.knn[class.knn=="No"] 

# run the HMeasure function on the data frame of scores
scores &lt;- data.frame(LDA=scores.lda,kNN=scores.knn)
results &lt;- HMeasure(true.class,scores)

# report aggregate metrics
summary(results)
# additionally report threshold-specific metrics
summary(results,show.all=TRUE)

# experiment with fixing the sensitivity (resp. specificity)
summary(HMeasure(true.class,scores,level=c(0.95,0.99)))



</pre>

<hr /><div style="text-align: center;">[Package <em>hmeasure</em> version 1.0-2 <a href="00Index.html">Index</a>]</div>
</body></html>
